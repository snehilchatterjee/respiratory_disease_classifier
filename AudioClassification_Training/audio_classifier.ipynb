{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f12a315-5e97-4026-a985-0d1c9fca9971",
   "metadata": {},
   "source": [
    "### Importing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c861209b-b406-4ceb-88f0-c4cda64deae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "#Importing audio file paths\n",
    "import os\n",
    "\n",
    "#For managing dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#PyTorch\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "\n",
    "# Misc.\n",
    "import multiprocessing # will be used for loading data using multipler workers (cpu_count)\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Selecting device\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e9933-d394-4654-b4a6-c282b59dd9e5",
   "metadata": {},
   "source": [
    "### Recursively get all audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6a0daf-3d9d-4e91-ad47-f0e65286bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_fetch(src,audio_paths):\n",
    "    l=os.listdir(src)\n",
    "    if(len(l)!=0):\n",
    "        for i in range(len(l)):\n",
    "            if(\".wav\" in l[i] or \".mp3\" in l[i] or \".aac\" in l[i]):\n",
    "                audio_paths.append(str(src+l[i]))\n",
    "            elif(\".\" not in l[i]):\n",
    "                try:\n",
    "                    audio_fetch(str(src+\"/\"+l[i]+\"/\"),audio_paths)\n",
    "                except:\n",
    "                    continue\n",
    "                            \n",
    "# For getting all possible classes along with their label encoding as a dictionary\n",
    "def class_fetch(df: pd.DataFrame())->(list,dict):\n",
    "    s=set(sorted(df[1].unique()))\n",
    "    \n",
    "    toDel=[i for i in dict(df[1].value_counts(sort=True)).keys() if dict(df[1].value_counts(sort=True))[i]<3]\n",
    "    for i in toDel:\n",
    "        s.remove(i)\n",
    "    d={}\n",
    "    \n",
    "    toDel_ids=[]\n",
    "    for i in toDel:\n",
    "        toDel_ids.extend(list(df[df[1]==i][0].values))\n",
    "    \n",
    "    \n",
    "    count=0\n",
    "    for i in s:\n",
    "        d[i]=count\n",
    "        count+=1\n",
    "    return list(s),d, toDel_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2801d1-6044-40a8-aefa-74ab85dee3af",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3c73728-c4cd-43b5-9208-2d3b649319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class audio_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 info: str,\n",
    "                 audio_path=None,\n",
    "                 audio_files=None,\n",
    "                 target_sample_rate=16000,\n",
    "                 num_samples=320000,\n",
    "                 transforms=None,\n",
    "                 header=None) -> None:\n",
    "        \n",
    "        self.paths=[]\n",
    "\n",
    "        if (audio_path==None and audio_files==None):\n",
    "            raise Exception(\"Both audio_path and audio_files cannot be None at the same time\")\n",
    "\n",
    "        if(audio_files==None):\n",
    "            audio_fetch(audio_path,self.paths)\n",
    "        else:\n",
    "            self.paths=audio_files\n",
    "\n",
    "        self.transformation=transforms\n",
    "        self.target_sample_rate=target_sample_rate\n",
    "        self.target_samples=num_samples\n",
    "        \n",
    "        self.patient=dict()\n",
    "        self.paths.sort() \n",
    "        \n",
    "        # We are going to remove all classes than 2 patient data\n",
    "\n",
    "        for i in self.paths:\n",
    "            index=int(i.split(\"/\")[-1].split(\"_\")[0])\n",
    "            self.patient[index]=[]\n",
    "        \n",
    "        for i in range(len(self.paths)):\n",
    "            self.patient[int(self.paths[i].split(\"/\")[-1].split(\"_\")[0])].append(self.paths[i])\n",
    "        \n",
    "        self.info_df=pd.read_csv(info,header=header)\n",
    "        self.classes, self.class_to_idx, self.ids_to_remove = class_fetch(self.info_df)\n",
    "\n",
    "        # After getting the patient ids to remove all the data from patient list is removed\n",
    "        for i in self.ids_to_remove:\n",
    "            if i in self.patient.keys():\n",
    "                self.patient.pop(i)\n",
    "\n",
    "        #Denotes sequence of audio files for a particular patient\n",
    "        self.sequence=list(self.patient.values())\n",
    "\n",
    "        # Audio path is updated after removing classing with small number of patients\n",
    "        self.paths=[]\n",
    "        for i in self.sequence:\n",
    "            if(i!=[]):\n",
    "                for j in i:\n",
    "                    self.paths.append(j)\n",
    "\n",
    "    \n",
    "    def get_class(self,file: str):\n",
    "        return self.info_df[self.info_df[0]==int(file.split(\"/\")[-1].split(\"_\")[0])][1].values[0]\n",
    "\n",
    "\n",
    "    # Necessary audio transformations: \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.target_samples:\n",
    "            signal = signal[:, :self.target_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.target_samples:\n",
    "            num_missing_samples = self.target_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor():\n",
    "        self.val=self.paths[index]\n",
    "        self.class_val=self.class_to_idx[self.get_class(self.val)]\n",
    "\n",
    "        self.signal, self.sr = torchaudio.load(self.val)\n",
    "        self.signal = self._resample_if_necessary(self.signal, self.sr)\n",
    "        self.signal = self._mix_down_if_necessary(self.signal)\n",
    "        self.signal = self._cut_if_necessary(self.signal)\n",
    "        self.signal = self._right_pad_if_necessary(self.signal)\n",
    "        if(self.transformation!=None):\n",
    "            self.signal = self.transformation(self.signal)\n",
    "\n",
    "        return self.signal,self.class_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d69c53-099d-4872-8995-1a22f40b5ffb",
   "metadata": {},
   "source": [
    "##### Testing audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ee9fbf4-8673-444f-bf7d-37167d40138a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pneumonia': 0,\n",
       " 'URTI': 1,\n",
       " 'Healthy': 2,\n",
       " 'Bronchiectasis': 3,\n",
       " 'Bronchiolitis': 4,\n",
       " 'COPD': 5}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=16000,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "\n",
    "ds=audio_dataset(audio_path=\"../Sound_Classification/archive/Respiratory_Sound_Database/\",info=\"../Sound_Classification/archive/respiratory_sound_database/Respiratory_Sound_Database/patient_diagnosis.csv\",transforms=mel_spectrogram)\n",
    "ds.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d59361e-6d17-4230-9894-30e277295574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "COPD\n"
     ]
    }
   ],
   "source": [
    "print(len(ds.sequence))\n",
    "print(ds.get_class(ds.paths[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad11d1-6b4a-47ed-a2b9-966513802e32",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ffe44aa-4c99-48f3-b5be-7c4aebdefaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trimming:\n",
      "No. of audio files in train_ds: 552\n",
      "No. of audio files in test_ds: 184\n",
      "No. of audio files in val_ds: 184\n",
      "\n",
      "After trimming:\n",
      "No. of audio files in train_ds: 550\n",
      "No. of audio files in test_ds: 184\n",
      "No. of audio files in val_ds: 183\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=4\n",
    "NUM_WORKERS=multiprocessing.cpu_count()\n",
    "\n",
    "ds=[]\n",
    "audio_fetch(\"../Sound_Classification/archive/respiratory_sound_database/\",ds)\n",
    "\n",
    "# Perform train test split\n",
    "train_ds,temp_ds=train_test_split(ds,test_size=0.4)\n",
    "test_ds,val_ds=train_test_split(temp_ds,test_size=0.5)\n",
    "\n",
    "print(f\"Before trimming:\")\n",
    "print(f\"No. of audio files in train_ds: {len(train_ds)}\\nNo. of audio files in test_ds: {len(test_ds)}\\nNo. of audio files in val_ds: {len(val_ds)}\")\n",
    "\n",
    "train_ds=audio_dataset(audio_files=train_ds,info=\"../Sound_Classification/archive/respiratory_sound_database/Respiratory_Sound_Database/patient_diagnosis.csv\",transforms=mel_spectrogram)\n",
    "test_ds=audio_dataset(audio_files=test_ds,info=\"../Sound_Classification/archive/respiratory_sound_database/Respiratory_Sound_Database/patient_diagnosis.csv\",transforms=mel_spectrogram)\n",
    "val_ds=audio_dataset(audio_files=val_ds,info=\"../Sound_Classification/archive/respiratory_sound_database/Respiratory_Sound_Database/patient_diagnosis.csv\",transforms=mel_spectrogram)\n",
    "\n",
    "print(f\"\\nAfter trimming:\")\n",
    "print(f\"No. of audio files in train_ds: {train_ds.__len__()}\\nNo. of audio files in test_ds: {test_ds.__len__()}\\nNo. of audio files in val_ds: {val_ds.__len__()}\")\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\n",
    "test_loader=torch.utils.data.DataLoader(test_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)\n",
    "val_loader=torch.utils.data.DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057acd7-5e6a-4e88-9759-502addf95d34",
   "metadata": {},
   "source": [
    "##### Checking dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ae0cf7d-392b-4756-a4d9-5ba707bd553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23326/3893681720.py:10: UserWarning: Only one segment is calculated since parameter NFFT (=256) >= signal length (=64).\n",
      "  plt.specgram(im)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4.06605179e+05],\n",
       "        [7.75414521e+05],\n",
       "        [6.71313353e+05],\n",
       "        [5.25484555e+05],\n",
       "        [3.69575247e+05],\n",
       "        [2.32866568e+05],\n",
       "        [1.35097941e+05],\n",
       "        [8.32006248e+04],\n",
       "        [7.18633745e+04],\n",
       "        [8.70442086e+04],\n",
       "        [1.11257776e+05],\n",
       "        [1.29143629e+05],\n",
       "        [1.31606678e+05],\n",
       "        [1.17256485e+05],\n",
       "        [9.10033753e+04],\n",
       "        [6.08401043e+04],\n",
       "        [3.43449042e+04],\n",
       "        [1.61707321e+04],\n",
       "        [7.13624443e+03],\n",
       "        [4.90705246e+03],\n",
       "        [5.77703247e+03],\n",
       "        [6.73407418e+03],\n",
       "        [6.91622249e+03],\n",
       "        [7.85272442e+03],\n",
       "        [1.24709057e+04],\n",
       "        [2.34301714e+04],\n",
       "        [4.15960240e+04],\n",
       "        [6.52988324e+04],\n",
       "        [9.06074137e+04],\n",
       "        [1.12439046e+05],\n",
       "        [1.26071857e+05],\n",
       "        [1.28541536e+05],\n",
       "        [1.19472818e+05],\n",
       "        [1.01102226e+05],\n",
       "        [7.75375011e+04],\n",
       "        [5.35533001e+04],\n",
       "        [3.33334030e+04],\n",
       "        [1.95222230e+04],\n",
       "        [1.28093188e+04],\n",
       "        [1.21025084e+04],\n",
       "        [1.51726379e+04],\n",
       "        [1.95084895e+04],\n",
       "        [2.30698649e+04],\n",
       "        [2.47120053e+04],\n",
       "        [2.42275683e+04],\n",
       "        [2.21065570e+04],\n",
       "        [1.91780394e+04],\n",
       "        [1.62775430e+04],\n",
       "        [1.40264984e+04],\n",
       "        [1.27478982e+04],\n",
       "        [1.24908510e+04],\n",
       "        [1.31092411e+04],\n",
       "        [1.43425795e+04],\n",
       "        [1.58700359e+04],\n",
       "        [1.73362635e+04],\n",
       "        [1.83726048e+04],\n",
       "        [1.86499579e+04],\n",
       "        [1.79801084e+04],\n",
       "        [1.64279233e+04],\n",
       "        [1.43505030e+04],\n",
       "        [1.22993978e+04],\n",
       "        [1.08100366e+04],\n",
       "        [1.01858492e+04],\n",
       "        [1.03888983e+04],\n",
       "        [1.10776305e+04],\n",
       "        [1.17548203e+04],\n",
       "        [1.19580755e+04],\n",
       "        [1.14325383e+04],\n",
       "        [1.02354257e+04],\n",
       "        [8.72813381e+03],\n",
       "        [7.43718900e+03],\n",
       "        [6.82149304e+03],\n",
       "        [7.04374537e+03],\n",
       "        [7.86437871e+03],\n",
       "        [8.73227256e+03],\n",
       "        [9.05318979e+03],\n",
       "        [8.52158251e+03],\n",
       "        [7.35642923e+03],\n",
       "        [6.31389067e+03],\n",
       "        [6.44446263e+03],\n",
       "        [8.67475910e+03],\n",
       "        [1.33746999e+04],\n",
       "        [2.00878048e+04],\n",
       "        [2.75483870e+04],\n",
       "        [3.40026598e+04],\n",
       "        [3.77318628e+04],\n",
       "        [3.75952198e+04],\n",
       "        [3.34044147e+04],\n",
       "        [2.60084978e+04],\n",
       "        [1.70735852e+04],\n",
       "        [8.63897348e+03],\n",
       "        [2.58749373e+03],\n",
       "        [1.74791007e+02],\n",
       "        [1.73197994e+03],\n",
       "        [6.60793129e+03],\n",
       "        [1.33630796e+04],\n",
       "        [2.01675012e+04],\n",
       "        [2.52960335e+04],\n",
       "        [2.75730733e+04],\n",
       "        [2.66293837e+04],\n",
       "        [2.29028310e+04],\n",
       "        [1.74144109e+04],\n",
       "        [1.14265822e+04],\n",
       "        [6.10703079e+03],\n",
       "        [2.28466885e+03],\n",
       "        [3.31712113e+02],\n",
       "        [1.66151079e+02],\n",
       "        [1.34877647e+03],\n",
       "        [3.23917891e+03],\n",
       "        [5.16994913e+03],\n",
       "        [6.60004405e+03],\n",
       "        [7.21932734e+03],\n",
       "        [6.99205427e+03],\n",
       "        [6.13954507e+03],\n",
       "        [5.06852498e+03],\n",
       "        [4.25627207e+03],\n",
       "        [4.11278970e+03],\n",
       "        [4.85282050e+03],\n",
       "        [6.41753388e+03],\n",
       "        [8.47708319e+03],\n",
       "        [1.05195865e+04],\n",
       "        [1.20002162e+04],\n",
       "        [1.25023397e+04],\n",
       "        [1.18611956e+04],\n",
       "        [1.02165520e+04],\n",
       "        [7.98280939e+03],\n",
       "        [5.74434716e+03],\n",
       "        [4.10015640e+03],\n",
       "        [1.74850861e+03]]),\n",
       " array([0.       , 0.0078125, 0.015625 , 0.0234375, 0.03125  , 0.0390625,\n",
       "        0.046875 , 0.0546875, 0.0625   , 0.0703125, 0.078125 , 0.0859375,\n",
       "        0.09375  , 0.1015625, 0.109375 , 0.1171875, 0.125    , 0.1328125,\n",
       "        0.140625 , 0.1484375, 0.15625  , 0.1640625, 0.171875 , 0.1796875,\n",
       "        0.1875   , 0.1953125, 0.203125 , 0.2109375, 0.21875  , 0.2265625,\n",
       "        0.234375 , 0.2421875, 0.25     , 0.2578125, 0.265625 , 0.2734375,\n",
       "        0.28125  , 0.2890625, 0.296875 , 0.3046875, 0.3125   , 0.3203125,\n",
       "        0.328125 , 0.3359375, 0.34375  , 0.3515625, 0.359375 , 0.3671875,\n",
       "        0.375    , 0.3828125, 0.390625 , 0.3984375, 0.40625  , 0.4140625,\n",
       "        0.421875 , 0.4296875, 0.4375   , 0.4453125, 0.453125 , 0.4609375,\n",
       "        0.46875  , 0.4765625, 0.484375 , 0.4921875, 0.5      , 0.5078125,\n",
       "        0.515625 , 0.5234375, 0.53125  , 0.5390625, 0.546875 , 0.5546875,\n",
       "        0.5625   , 0.5703125, 0.578125 , 0.5859375, 0.59375  , 0.6015625,\n",
       "        0.609375 , 0.6171875, 0.625    , 0.6328125, 0.640625 , 0.6484375,\n",
       "        0.65625  , 0.6640625, 0.671875 , 0.6796875, 0.6875   , 0.6953125,\n",
       "        0.703125 , 0.7109375, 0.71875  , 0.7265625, 0.734375 , 0.7421875,\n",
       "        0.75     , 0.7578125, 0.765625 , 0.7734375, 0.78125  , 0.7890625,\n",
       "        0.796875 , 0.8046875, 0.8125   , 0.8203125, 0.828125 , 0.8359375,\n",
       "        0.84375  , 0.8515625, 0.859375 , 0.8671875, 0.875    , 0.8828125,\n",
       "        0.890625 , 0.8984375, 0.90625  , 0.9140625, 0.921875 , 0.9296875,\n",
       "        0.9375   , 0.9453125, 0.953125 , 0.9609375, 0.96875  , 0.9765625,\n",
       "        0.984375 , 0.9921875, 1.       ]),\n",
       " array([64.]),\n",
       " <matplotlib.image.AxesImage at 0x7f452df0b7d0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im=next(iter(train_loader))[0][0]\n",
    "im=im.permute(1,2,0)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(y,ref=np.max),\n",
    "                             y_axis='mel',\n",
    "                             fmax=8000,\n",
    "                             x_axis='time')'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.specgram(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22aae5f5-c4d9-44a1-ac3a-36a017ed33dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 44, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5de65-db6c-4004-a018-7a2a65b43835",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c83bc18-b8f6-41e9-afb9-da5daa7d9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class auditory_transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_embedding=nn.Sequential(nn.BatchNorm1d(num_features=1),\n",
    "                                           nn.Conv1d(in_channels=1,out_channels=128,kernel_size=3),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.BatchNorm1d(num_features=128),\n",
    "                                           nn.Conv1d(in_channels=128,out_channels=256,kernel_size=3),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.BatchNorm1d(num_features=256),\n",
    "                                           nn.Conv1d(in_channels=256,out_channels=512,kernel_size=3),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.BatchNorm1d(num_features=512),\n",
    "                                           nn.Conv1d(in_channels=512,out_channels=1024,kernel_size=3),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.BatchNorm1d(num_features=1024),\n",
    "                                           nn.Conv1d(in_channels=1024,out_channels=2048,kernel_size=3),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.BatchNorm1d(num_features=2048),\n",
    "                                           nn.Conv1d(in_channels=2048,out_channels=4096,kernel_size=3))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.input_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e84ad23-c948-4730-9e56-55a6825b9bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 80000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c814e529-32c3-4470-b631-8227b0ee741e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacty of 5.80 GiB of which 3.07 GiB is free. Including non-PyTorch memory, this process has 2.71 GiB memory in use. Of the allocated memory 2.57 GiB is allocated by PyTorch, and 12.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m=\u001b[39mauditory_transformer()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mauditory_transformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacty of 5.80 GiB of which 3.07 GiB is free. Including non-PyTorch memory, this process has 2.71 GiB memory in use. Of the allocated memory 2.57 GiB is allocated by PyTorch, and 12.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model=auditory_transformer().to(device)\n",
    "with torch.inference_mode():\n",
    "    model((next(iter(train_loader))[0]).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a9def-1d49-4f91-a497-49eb1fd2a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, sr=torchaudio.load(train_ds.paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2da0f-2d2e-4453-a790-05e9616dd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resmaple=torchaudio.transforms.Resample(torchaudio.load(train_ds.paths[0])[1],16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f184ef3-8336-459b-992c-611d39df4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resmaple(wv).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aaeabb-d74d-4735-a47a-727bd4f43975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(next(iter(train_loader))[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf462b6-5114-43f8-b3a1-9fd6129631ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
